{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search Multimodal Retrieval Demo\n",
    "\n",
    "## Introduction\n",
    "In this demo, we will show you how to create a multimodal (text + images) vector index in Azure AI Search.\n",
    "\n",
    "## Prerequisites\n",
    "- üêç Python 3.9 or higher\n",
    "- ‚òÅÔ∏è Azure Blob Storage\n",
    "- üîó Azure AI Vision Service or Azure AI Multi-Service Account\n",
    "- üîó Azure AI Search Service\n",
    "\n",
    "## Features Covered\n",
    "This demo covers the following features:\n",
    "- ‚úÖ Stored=False\n",
    "- ‚úÖ Scalar Quantization to int8\n",
    "- ‚úÖ Reranking w/full precision vectors\n",
    "- ‚úÖ Oversampling\n",
    "- ‚úÖ Integrated Vectorization\n",
    "- ‚úÖ Multi-Vector Search\n",
    "- ‚úÖ Generate Dense Captions with AI Vision Image Analysis API\n",
    "- ‚úÖ **[NEW]** Azure AI Vision Embedding Skill\n",
    "- ‚úÖ **[NEW]** Azure AI Vision Vectorizer\n",
    "- ‚úÖ **[NEW]** Azure AI Vision Latest Multilingual Model\n",
    "- ‚úÖ **[NEW]** Vector Weighting\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install azure-search-documents --pre --quiet\n",
    "#%pip install azure-search-documents==11.6.0b4\n",
    "#%pip install openai python-dotenv azure-identity cohere azure-ai-vision-imageanalysis --quiet\n",
    "#%pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path \n",
    "import os\n",
    "\n",
    "env_path = Path('.') / 'secrets.env'\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI, OpenAI\n",
    "import base64\n",
    "\n",
    "\n",
    "endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "api_key = os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "# set the deployment name for the model we want to use\n",
    "deployment = os.environ[\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\"]\n",
    "model: str = \"text-embedding-ada-002\" \n",
    "\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
    "\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "  \n",
    "\n",
    "\n",
    "def get_chatgpt_base_response_with_image(userQuery, system_message, image_path, client_type=\"openai\", base64_image=None, model=None, max_tokens=4000):\n",
    "    \n",
    "    if base64_image is None:\n",
    "        base64_image = encode_image(image_path)\n",
    "    \n",
    "    \n",
    "    if model is None:\n",
    "        model = os.getenv('DEPLOYMENT_NAME')\n",
    "\n",
    "    print(f\"model used:{model}\")\n",
    "\n",
    "    client = None\n",
    "    if client_type == \"openai\":\n",
    "        client = openai_client\n",
    "        print(\"openai client used\")\n",
    "    else:\n",
    "        client = azure_client\n",
    "        print(\"azure client used\")\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": userQuery},\n",
    "                {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    \"detail\": \"high\"\n",
    "                },\n",
    "                },\n",
    "            ],\n",
    "            }\n",
    "        ],\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        temperature=0,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=False)\n",
    "    \n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files in path downloads\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dir_path = \"downloads\"\n",
    "\n",
    "system_message = \"\"\"You are an AI assistant that describe images in detail\n",
    "Sample Json format:\n",
    "{\n",
    "    \"image\": \"image_path\",\n",
    "    \"description\": \"image description in detail.\" \n",
    "    \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for f in listdir(dir_path):\n",
    "    # print full path\n",
    "    image_path = join(dir_path, f)\n",
    "    response = get_chatgpt_base_response_with_image(system_message, f\"describe the attached image in detail in json format. image_path: {image_path}\", image_path, client_type=\"openai\", model=\"gpt-4o\", max_tokens=200)\n",
    "    print(response)\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "def get_image_description(base64_img):\n",
    "    response = get_chatgpt_base_response_with_image(system_message, f\"describe the attached image in detail in json format. image_path: {image_path}\", image_path=None, client_type=\"openai\", base64_image=base64_img, model=\"gpt-4o\", max_tokens=200)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "#load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "AZURE_AI_VISION_API_KEY = os.getenv(\"AZURE_AI_VISION_KEY\")\n",
    "AZURE_AI_VISION_ENDPOINT = os.getenv(\"AZURE_AI_VISION_ENDPOINT\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "BLOB_CONNECTION_STRING = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "BLOB_CONTAINER_NAME = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "SEARCH_BLOB_CONTAINER = os.getenv(\"SEARCH_BLOB_CONTAINER\")\n",
    "INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "SEARCH_SERVICE_API_KEY = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSON with text and images via Azure AI Vision Studio\n",
    "https://portal.vision.cognitive.azure.com/demo/image-captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Get environment variables for Azure AI Vision\n",
    "try:\n",
    "    endpoint = os.getenv(\"AZURE_AI_VISION_ENDPOINT\")\n",
    "    key = os.getenv(\"AZURE_AI_VISION_KEY\")\n",
    "    connection_string = os.getenv(\"BLOB_CONNECTION_STRING\")\n",
    "    container_name = os.getenv(\"BLOB_CONTAINER_NAME\")\n",
    "    #container_name = \"vector-sandbox\"\n",
    "except KeyError as e:\n",
    "    print(f\"Missing environment variable: {str(e)}\")\n",
    "    print(\"Set them before running this sample.\")\n",
    "    exit()\n",
    "\n",
    "# Create an Image Analysis client\n",
    "client = ImageAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "# Setup for Azure Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload files to blob storage from local blob_files folder\n",
    "\n",
    "local_images_folder = \"blob_files\"\n",
    "\n",
    "def upload_files_to_blob_storage():\n",
    "    # Get all files in the blob_files folder\n",
    "    blob_files = [f for f in listdir(local_images_folder) if isfile(join(local_images_folder, f))]\n",
    "\n",
    "    # Upload each file to the blob storage\n",
    "    for file in blob_files:\n",
    "        blob_client = container_client.get_blob_client(file)\n",
    "        with open(f\"{local_images_folder}/{file}\", \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "            print(f\"Uploaded {file} to blob storage\")\n",
    "\n",
    "\n",
    "upload_files_to_blob_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "for blob in container_client.list_blobs():\n",
    "    print(f\"Analyzing {blob.name}\")\n",
    "    # read the image from the blob storage\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    image = blob_client.download_blob().readall()\n",
    "    base64_img = base64.b64encode(image).decode('utf-8')\n",
    "    img_description = get_image_description(base64_img)\n",
    "    print(json.loads(img_description)[\"description\"])\n",
    "    break\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_token = \"?\" # Add the Azure Blob Storage SAS token here\n",
    "\n",
    "def get_caption(image_url):\n",
    "    \"\"\"\n",
    "    Get a caption for the image using Azure AI Vision.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = client.analyze_from_url(\n",
    "            image_url=image_url,\n",
    "            visual_features=[VisualFeatures.CAPTION, VisualFeatures.READ],\n",
    "            gender_neutral_caption=False\n",
    "        )\n",
    "        if result.caption is not None:\n",
    "            return result.caption.text\n",
    "        else:\n",
    "            return \"No caption available\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"Error generating caption\"\n",
    "\n",
    "def generate_json_objects():\n",
    "    json_objects = []\n",
    "\n",
    "    # Iterate over the blobs in the container\n",
    "    for blob in container_client.list_blobs():\n",
    "        image_url = f\"https://{blob_service_client.account_name}.blob.core.windows.net/{container_name}/{blob.name}{sas_token}\"\n",
    "        caption = get_caption(image_url)\n",
    "\n",
    "        print(f\"Analyzing {blob.name}\")\n",
    "        # read the image from the blob storage\n",
    "        blob_client = container_client.get_blob_client(blob.name)\n",
    "        image = blob_client.download_blob().readall()\n",
    "        base64_img = base64.b64encode(image).decode('utf-8')\n",
    "        img_description = get_image_description(base64_img)\n",
    "        json_object = {\"id\": str(uuid4()), \"imageUrl\": image_url, \"caption\": caption, \"imageDescription\": json.loads(img_description)[\"description\"]}\n",
    "        json_objects.append(json_object)\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "def write_to_file(json_objects):\n",
    "    # Write the updated JSON to a file\n",
    "    with open(\"build-demo.json\", \"w\") as json_file:\n",
    "        json.dump(json_objects, json_file, indent=4)\n",
    "\n",
    "json_objects = generate_json_objects()\n",
    "write_to_file(json_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient  \n",
    "import glob\n",
    "\n",
    "def upload_sample_documents(\n",
    "        blob_connection_string: str,\n",
    "        blob_container_name: str,\n",
    "        use_user_identity: bool = True\n",
    "    ):\n",
    "    # Connect to Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=blob_connection_string, credential=DefaultAzureCredential() if use_user_identity else None)\n",
    "    container_client = blob_service_client.get_container_client(blob_container_name)\n",
    "    if not container_client.exists():\n",
    "        container_client.create_container()\n",
    "\n",
    "    documents_directory = \".\"\n",
    "    csv_files = glob.glob(os.path.join(documents_directory, '*.json'))\n",
    "    for file in csv_files:\n",
    "        with open(file, \"rb\") as data:\n",
    "            name = os.path.basename(file)\n",
    "            container_client.upload_blob(name=name, data=data, overwrite=True)\n",
    "            #if not container_client.get_blob_client(name).exists():\n",
    "            #    container_client.upload_blob(name=name, data=data)\n",
    "\n",
    "upload_sample_documents(\n",
    "    blob_connection_string=BLOB_CONNECTION_STRING,\n",
    "    blob_container_name=SEARCH_BLOB_CONTAINER,\n",
    "    # Set to false if you want to use credentials included in the blob connection string\n",
    "    # Otherwise your identity will be used as credentials\n",
    "    use_user_identity=False\n",
    ")\n",
    "print(f\"Setup sample data in {SEARCH_BLOB_CONTAINER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    AIServicesVisionParameters,\n",
    "    AIServicesVisionVectorizer,\n",
    "    AIStudioModelCatalogName,\n",
    "    AzureMachineLearningVectorizer,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIModelName,\n",
    "    AzureOpenAIParameters,\n",
    "    BlobIndexerDataToExtract,\n",
    "    BlobIndexerParsingMode,\n",
    "    CognitiveServicesAccountKey,\n",
    "    DefaultCognitiveServicesAccount,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    FieldMapping,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    IndexerExecutionStatus,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    ScalarQuantizationCompressionConfiguration,\n",
    "    ScalarQuantizationParameters,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndex,\n",
    "    SearchIndexer,\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataIdentity,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SearchIndexerSkillset,\n",
    "    SemanticConfiguration,\n",
    "    SemanticField,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SimpleField,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    "    VisionVectorizeSkill\n",
    ")\n",
    "from azure.search.documents.models import (\n",
    "    HybridCountAndFacetMode,\n",
    "    HybridSearch,\n",
    "    SearchScoreThreshold,\n",
    "    VectorizableTextQuery,\n",
    "    VectorizableImageBinaryQuery,\n",
    "    VectorizableImageUrlQuery,\n",
    "    VectorSimilarityThreshold,\n",
    ")\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display, HTML\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AZURE_AI_VISION_ENDPOINT)\n",
    "print(AZURE_AI_VISION_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified parameter\n",
    "USE_AAD_FOR_SEARCH = False  # Set this to False to use API key for authentication\n",
    "\n",
    "def authenticate_azure_search(api_key=None, use_aad_for_search=False):\n",
    "    if use_aad_for_search:\n",
    "        print(\"Using AAD for authentication.\")\n",
    "        credential = DefaultAzureCredential()\n",
    "    else:\n",
    "        print(\"Using API keys for authentication.\")\n",
    "        if api_key is None:\n",
    "            raise ValueError(\"API key must be provided if not using AAD for authentication.\")\n",
    "        credential = AzureKeyCredential(api_key)\n",
    "    return credential\n",
    "\n",
    "azure_search_credential = authenticate_azure_search(api_key=SEARCH_SERVICE_API_KEY, use_aad_for_search=USE_AAD_FOR_SEARCH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a blob data source connector on Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection,\n",
    "    SoftDeleteColumnDeletionDetectionPolicy\n",
    ")\n",
    "\n",
    "credential = AzureKeyCredential(os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")) if os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") else DefaultAzureCredential()\n",
    "\n",
    "\n",
    "# Create a data source\n",
    "# NOTE: To remove records from a search index, add a column to the row \"IsDeleted\" set to \"True\". The next indexer run will remove this record\n",
    "# To learn more please visit https://learn.microsoft.com/en-us/azure/search/search-howto-index-one-to-many-blobs\n",
    "indexer_client = SearchIndexerClient(SEARCH_SERVICE_ENDPOINT, credential)\n",
    "container = SearchIndexerDataContainer(name=SEARCH_BLOB_CONTAINER)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=f\"{INDEX_NAME}-blob\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=BLOB_CONNECTION_STRING,\n",
    "    container=container,\n",
    "    data_deletion_detection_policy=SoftDeleteColumnDeletionDetectionPolicy(soft_delete_column_name=\"IsDeleted\", soft_delete_marker_value=\"True\")\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields():\n",
    "    \"\"\"Creates the fields for the search index based on the specified schema.\"\"\"\n",
    "    return [\n",
    "        SimpleField(\n",
    "            name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True\n",
    "        ),\n",
    "        SearchField(name=\"caption\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"imageUrl\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(name=\"imageDescription\", type=SearchFieldDataType.String, searchable=True),\n",
    "        SearchField(\n",
    "            name=\"imageDescriptionVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "            stored=False,\n",
    "            retrievable=True\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"captionVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "            stored=False,\n",
    "            retrievable=True\n",
    "        ),\n",
    "        SearchField(\n",
    "            name=\"imageVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            vector_search_dimensions=1024,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "            stored=False,\n",
    "            retrievable=True\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_vector_search_configuration():\n",
    "    \"\"\"Creates the vector search configuration.\"\"\"\n",
    "    return VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=400,\n",
    "                    ef_search=500,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        compressions=[\n",
    "            ScalarQuantizationCompressionConfiguration(\n",
    "                name=\"myScalarQuantization\",\n",
    "                rerank_with_original_vectors=True,\n",
    "                default_oversampling=10,\n",
    "                parameters=ScalarQuantizationParameters(quantized_data_type=\"int8\"),\n",
    "            )\n",
    "        ],\n",
    "        vectorizers=[\n",
    "            AIServicesVisionVectorizer(\n",
    "                name=\"myAIServicesVectorizer\",\n",
    "                kind=\"aiServicesVision\",\n",
    "                ai_services_vision_parameters=AIServicesVisionParameters(\n",
    "                    model_version=\"2023-04-15\",\n",
    "                    resource_uri=AZURE_AI_VISION_ENDPOINT,\n",
    "                    api_key=AZURE_AI_VISION_API_KEY,\n",
    "                ),\n",
    "            )\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "                compression_configuration_name=\"myScalarQuantization\",\n",
    "                vectorizer=\"myAIServicesVectorizer\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def create_search_index(index_client, index_name, fields, vector_search):\n",
    "    \"\"\"Creates or updates a search index.\"\"\"\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "    )\n",
    "    index_client.create_or_update_index(index=index)\n",
    "\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT, credential=azure_search_credential\n",
    ")\n",
    "fields = create_fields()\n",
    "vector_search = create_vector_search_configuration()\n",
    "\n",
    "# Create the search index with the adjusted schema\n",
    "create_search_index(index_client, INDEX_NAME, fields, vector_search)\n",
    "print(f\"Created index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Skillset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AZURE_AI_VISION_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_embedding_skill():\n",
    "    return VisionVectorizeSkill(\n",
    "        name=\"text-embedding-skill\",\n",
    "        description=\"Skill to generate embeddings for text via Azure AI Vision\",\n",
    "        context=\"/document\",\n",
    "        model_version=\"2023-04-15\",\n",
    "        inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/caption\")],\n",
    "        outputs=[OutputFieldMappingEntry(name=\"vector\", target_name=\"captionVector\")],\n",
    "    )\n",
    "\n",
    "def create_image_description_text_embedding_skill():\n",
    "    return VisionVectorizeSkill(\n",
    "        name=\"image-description-text-embedding-skill\",\n",
    "        description=\"Skill to generate embeddings for text via Azure AI Vision\",\n",
    "        context=\"/document\",\n",
    "        model_version=\"2023-04-15\",\n",
    "        inputs=[InputFieldMappingEntry(name=\"text\", source=\"/document/imageDescription\")],\n",
    "        outputs=[OutputFieldMappingEntry(name=\"vector\", target_name=\"imageDescriptionVector\")],\n",
    "    )\n",
    "\n",
    "def create_image_embedding_skill():\n",
    "    return VisionVectorizeSkill(\n",
    "        name=\"image-embedding-skill\",\n",
    "        description=\"Skill to generate embeddings for image via Azure AI Vision\",\n",
    "        context=\"/document\",\n",
    "        model_version=\"2023-04-15\",\n",
    "        inputs=[InputFieldMappingEntry(name=\"url\", source=\"/document/imageUrl\")],\n",
    "        outputs=[OutputFieldMappingEntry(name=\"vector\", target_name=\"imageVector\")],\n",
    "    )\n",
    "\n",
    "def create_skillset(client, skillset_name, text_embedding_skill, image_description_text_embedding_skill, image_embedding_skill):\n",
    "    skillset = SearchIndexerSkillset(\n",
    "        name=skillset_name,\n",
    "        description=\"Skillset for generating embeddings\",\n",
    "        skills=[text_embedding_skill, image_description_text_embedding_skill, image_embedding_skill],\n",
    "        cognitive_services_account=CognitiveServicesAccountKey(\n",
    "            key=AZURE_AI_VISION_API_KEY,\n",
    "            description=\"AI Vision Multi Service Account in West US\",\n",
    "        ),\n",
    "    )\n",
    "    client.create_or_update_skillset(skillset)\n",
    "\n",
    "client = SearchIndexerClient(\n",
    "    endpoint=SEARCH_SERVICE_ENDPOINT, credential=azure_search_credential\n",
    ")\n",
    "skillset_name = f\"{INDEX_NAME}-skillset\"\n",
    "text_embedding_skill = create_text_embedding_skill()\n",
    "image_description_text_embedding_skill = create_image_description_text_embedding_skill()\n",
    "image_embedding_skill = create_image_embedding_skill()\n",
    "\n",
    "create_skillset(client, skillset_name, text_embedding_skill, image_description_text_embedding_skill, image_embedding_skill)\n",
    "print(f\"Created skillset: {skillset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    FieldMapping,\n",
    "    FieldMappingFunction,\n",
    "    IndexingParameters,\n",
    "    IndexingParametersConfiguration,\n",
    "    BlobIndexerParsingMode\n",
    ")\n",
    "\n",
    "# Create an indexer  \n",
    "indexer_name = f\"{INDEX_NAME}-indexer\"  \n",
    "indexer_parameters = IndexingParameters(\n",
    "        configuration=IndexingParametersConfiguration(\n",
    "            parsing_mode=BlobIndexerParsingMode.JSON_ARRAY,\n",
    "            query_timeout=None,\n",
    "           ))\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=INDEX_NAME,  \n",
    "    data_source_name=data_source.name,\n",
    "    parameters=indexer_parameters,\n",
    "    field_mappings=[FieldMapping(source_field_name=\"id\", target_field_name=\"id\")],\n",
    "    output_field_mappings=[\n",
    "        FieldMapping(source_field_name=\"/document/captionVector\", target_field_name=\"captionVector\"),\n",
    "        FieldMapping(source_field_name=\"/document/imageDescriptionVector\", target_field_name=\"imageDescriptionVector\"),\n",
    "        FieldMapping(source_field_name=\"/document/imageVector\", target_field_name=\"imageVector\")\n",
    "    ]\n",
    ")  \n",
    "\n",
    "indexer_client = SearchIndexerClient(SEARCH_SERVICE_ENDPOINT, credential)  \n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)  \n",
    "  \n",
    "# Run the indexer  \n",
    "indexer_client.run_indexer(indexer_name)  \n",
    "print(f'{indexer_name} is created and running. If queries return no results, please wait a bit and try again.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple vector search (text to text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SearchClient\n",
    "search_client = SearchClient(\n",
    "    SEARCH_SERVICE_ENDPOINT,\n",
    "    index_name=INDEX_NAME,\n",
    "    credential=azure_search_credential,\n",
    ")\n",
    "\n",
    "# Define the query\n",
    "# query = \"sunglasses for holiday\"\n",
    "# query = \"‰ºëÊó•„ÅÆ„Çµ„É≥„Ç∞„É©„Çπ\" # Japanese query\n",
    "query = \"trees with buildings\" # Spanish query\n",
    "\n",
    "vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=3,\n",
    "    fields=\"captionVector\",\n",
    "    # fields=\"imageVector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None,\n",
    "    vector_queries=[vector_query],\n",
    "    top=3\n",
    ")\n",
    "\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    display(HTML(f'<img src=\"{result[\"imageUrl\"]}\" style=\"width:200px;\"/>'))\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Search (text to image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text query\n",
    "query = \"city with buildings\"\n",
    "text_vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=10,\n",
    "    fields=\"captionVector\",\n",
    ")\n",
    "# Define the image query\n",
    "image_vector_query = VectorizableImageUrlQuery(  # Alternatively, use VectorizableImageBinaryQuery\n",
    "    url=\"https://media.gettyimages.com/id/155422469/photo/office-skysraper-in-the-sun.jpg?s=1024x1024&w=gi&k=20&c=E32XYAydthNC2NY59OqU2PzGes_i40E8aywKIgtnSBI=\",  #skyscrapper\n",
    "    k_nearest_neighbors=10,\n",
    "    fields=\"imageVector\",\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None, vector_queries=[text_vector_query, image_vector_query], top=3\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"URL: {result['imageUrl']}\")\n",
    "    display(HTML(f'<img src=\"{result[\"imageUrl\"]}\" style=\"width:200px;\"/>'))\n",
    "    print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-modal vector search with weighting images 100x more than captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text query\n",
    "query = \"city with trees an buildings\"\n",
    "text_vector_query = VectorizableTextQuery(\n",
    "    text=query,\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=\"captionVector\",\n",
    ")\n",
    "# Define the image query\n",
    "image_vector_query = VectorizableImageUrlQuery(  # Alternatively, use VectorizableImageBinaryQuery\n",
    "    url=\"https://media.gettyimages.com/id/1326704523/photo/sunrise-skyline-view-of-midtown-manhattan-and-lower-manhattan.jpg?s=1024x1024&w=gi&k=20&c=VWOJfHBYc0YGRYqN1vAysD6KXsYJqf3s-afHa8tl9dY=\",  # New York skyline\n",
    "    k_nearest_neighbors=5,\n",
    "    fields=\"imageVector\",\n",
    "    weight=100,\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "results = search_client.search(\n",
    "    search_text=None, vector_queries=[text_vector_query, image_vector_query], top=3\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Caption: {result['caption']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"URL: {result['imageUrl']}\")\n",
    "    display(HTML(f'<img src=\"{result[\"imageUrl\"]}\" style=\"width:200px;\"/>'))\n",
    "    print(\"-\" * 50)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
